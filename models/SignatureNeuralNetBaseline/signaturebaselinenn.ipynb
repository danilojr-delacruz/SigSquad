{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:22:52.506751Z","iopub.status.busy":"2024-03-19T17:22:52.506356Z","iopub.status.idle":"2024-03-19T17:24:38.047435Z","shell.execute_reply":"2024-03-19T17:24:38.046087Z","shell.execute_reply.started":"2024-03-19T17:22:52.506720Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: python: command not found\n"]}],"source":["! python -m pip install --no-index --find-links=../input/requirements -r ../input/requirements/requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["# utils.py"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:24:38.050291Z","iopub.status.busy":"2024-03-19T17:24:38.049926Z","iopub.status.idle":"2024-03-19T17:24:41.645243Z","shell.execute_reply":"2024-03-19T17:24:41.644237Z","shell.execute_reply.started":"2024-03-19T17:24:38.050257Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/barbora/.local/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n","Install h5py to use hdf5 features: http://docs.h5py.org/\n","  warn(h5py_msg)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tslearn.preprocessing import TimeSeriesScalerMinMax, TimeSeriesScalerMeanVariance\n","from scipy.signal import butter, lfilter\n","import signatory\n","import torch\n","\n","RESIDUAL_PAIRS = {'LP': [('Fp1', 'F3'), ('F3', 'C3'), ('C3', 'P3'), ('P3', 'O1')], \n","                  'RP': [('Fp2', 'F4'), ('F4', 'C4'), ('C4', 'P4'), ('P4', 'O2')], \n","                  'LT': [('Fp1', 'F7'), ('F7', 'T3'), ('T3', 'T5'), ('T5', 'O1')],\n","                  'RT': [('Fp2', 'F8'), ('F8', 'T4'), ('T4', 'T6'), ('T6', 'O2')],\n","                  # don't include the middle electrodes for now\n","                  #'C': [('Fz', 'Cz'), ('Cz', 'Pz')],\n","                  }\n","\n","TARGETS = [\n","    \"seizure_vote\",\t\"lpd_vote\", \"gpd_vote\",\n","    \"lrda_vote\", \"grda_vote\", \"other_vote\"\n","    ]\n","\n","def modify_metadata(metadata):\n","    \"\"\"Reduce the metadata to one data point per eeg_id (the one in the middle - median offset).\n","       Make the toal vote distribution across the reconding be the target.\n","       We are assuming that even though we have multiple sub-recordings, the true target value does not change.\n","    \"\"\"\n","    num_votes = metadata.iloc[:, -6:].sum(axis=1)\n","    metadata = metadata[(num_votes >= 10)]\n","    # note that other public notebooks calculate the offset differently, but I am not convinced it makes sense\n","    metadata_grouped = metadata.groupby(\"eeg_id\").agg(\n","        spectrogram_id     = pd.NamedAgg(\"spectrogram_id\", \"first\"),\n","        eeg_offset_seconds = pd.NamedAgg(\"eeg_label_offset_seconds\", \"median\"),\n","        spec_offset_seconds = pd.NamedAgg(\"spectrogram_label_offset_seconds\", \"median\"),\n","        patient_id         = pd.NamedAgg(\"patient_id\", \"first\"),\n","        target             = pd.NamedAgg(\"expert_consensus\", \"first\")\n","        )\n","    total_votes = metadata.groupby(\"eeg_id\")[TARGETS].agg(\"sum\")\n","    total_votes = total_votes.div(total_votes.sum(axis=1), axis=0)\n","    for vote_label in TARGETS:\n","        metadata_grouped[vote_label] = total_votes[vote_label]\n","\n","    return metadata_grouped.reset_index()\n","\n","def rescale(ts, scaler_type):\n","    \"\"\"Rescale the time series using the given type.\n","    \"\"\"\n","    if scaler_type == \"minmax\":\n","        scaler = TimeSeriesScalerMinMax()\n","        ts = scaler.fit_transform(ts)\n","    elif scaler_type.startswith(\"meanvarPerChannel\"):\n","        scaler_std = float(scaler_type.split(\"_\")[1])\n","        scaler = TimeSeriesScalerMeanVariance(std=scaler_std)\n","        ts = ts - ts.mean(axis=1, keepdims=True)\n","    elif scaler_type.startswith(\"constant\"):\n","        scaler_constant = float(scaler_type.split(\"_\")[1])\n","        ts = ts / scaler_constant\n","    elif scaler_type.startswith(\"meanvar\"):\n","        # this is done later since we atke the variance across all channels\n","        pass\n","    else:\n","        raise ValueError(f\"Unknown scaler type {scaler_type}\")\n","    return ts\n","\n","def transform_residuals(residuals, scaler_type):\n","    residuals = rescale(residuals.values.reshape(1,-1,1), scaler_type).reshape(-1)\n","    return residuals\n","\n","def get_residuals(eeg, scaler_type):\n","    \"\"\"Doctors look at the difference between two neighboring channels.\n","       Calculate the residuals for each channel pair.\n","       Group by brain region.\"\"\"\n","    brain_regions = []\n","    for region, pair in RESIDUAL_PAIRS.items():\n","        # include time as the first dimension and make it go from 0 to 1\n","        residuals = []\n","        for channel1, channel2 in pair:\n","            residual = transform_residuals(eeg[channel1] - eeg[channel2], scaler_type)\n","            residuals.append(residual)\n","        brain_regions.append(np.stack(residuals).T)\n","    brain_regions = np.stack(brain_regions)\n","\n","    if scaler_type.startswith(\"meanvar\"):\n","        brain_regions = brain_regions - brain_regions.mean(axis=1, keepdims=True)\n","        brain_regions = brain_regions / (brain_regions.std()+1e-6)\n","    return brain_regions.clip(-4, 4)\n","\n","def augment_with_time(residuals):\n","    \"\"\" take residuals of the shape (4, 10000, 4) and augment with time to obtain (4, 10000, 5)\"\"\"\n","    augmented_regions = []\n","    for region_index in range(4):\n","        augmented_regions.append(np.concatenate([residuals[region_index], np.linspace(0,1,10000).reshape(-1,1)], axis=1))\n","    return np.stack(augmented_regions)\n","\n","def butter_bandpass(lowcut, highcut, fs, order):\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","def butter_bandpass_filter(data, lowcut=0.1, highcut=30, fs=200, order=4):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data, axis=0)\n","    return y\n","\n","def preprocess_for_sig(metadata, data_dir, scaler_type):\n","    \"\"\"\"Preprocess the eeg data to feed into the logsignature function.\n","        The output tensor is of the shape (paths_to_calculate x  path_length = 10000 x path_dimensions = 5).\n","        paths to calculate = number_of_eeg_recordings * 4 brain regions for each recording.\n","    \"\"\"\n","    preprocessed = []\n","    for i, data in metadata.iterrows():\n","        eeg_id = data.eeg_id\n","        # eeg is sampled at 200 Hz\n","        offset = int(data.eeg_offset_seconds * 200 )\n","        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n","        eeg = pd.read_parquet(parquet_path)\n","        # replace 9999 with 0\n","        eeg = eeg.replace(9999, 0)\n","        eeg = eeg.fillna(0).clip(-100,100)\n","        eeg = eeg.iloc[offset:offset+10000]\n","        # bandpass filter\n","        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n","        residuals = get_residuals(eeg, scaler_type)\n","        residuals = augment_with_time(residuals)      \n","        preprocessed.append(residuals)\n","    preprocessed = np.concatenate(preprocessed, axis=0)\n","    \n","    return preprocessed\n","\n","def preprocess_for_sig_test(metadata, data_dir, scaler_type):\n","    \"\"\"Preprocessing needs to be different for the kaggle test set since we only have 50 second eeg recordings.\"\"\"\n","    preprocessed = []\n","    for i, data in metadata.iterrows():\n","        eeg_id = data.eeg_id\n","        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n","        eeg = pd.read_parquet(parquet_path)\n","        # replace 9999 with 0\n","        eeg = eeg.replace(9999, 0)\n","        eeg = eeg.fillna(0).clip(-100,100)\n","        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n","        residuals = get_residuals(eeg, scaler_type)\n","        residuals = augment_with_time(residuals)      \n","        preprocessed.append(residuals)\n","    preprocessed = np.concatenate(preprocessed, axis=0)\n","\n","    return preprocessed\n","\n","def calculate_logsignature(preprocessed, truncation_level):\n","    logsignature = signatory.logsignature(preprocessed, truncation_level)\n","    return logsignature\n","\n","def calculate_signature(preprocessed, truncation_level):\n","    signature = signatory.signature(preprocessed, truncation_level)\n","    return signature\n","\n","def calculate_logsignature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n","    \"\"\"Return the tensor of calculated signtures.\n","       Use this function to calculate the logsignatures for the kaggle test set.\n","    \"\"\"\n","    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n","    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n","    logsigs = calculate_logsignature(preprocessed, truncation_level=level).cpu()\n","    size = logsigs.shape[1]\n","    logsigs = logsigs.reshape(-1,4,size)\n","    return logsigs\n","\n","\n","def calculate_signature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n","    \"\"\"Return the tensor of calculated signtures.\n","       Use this function to calculate the signatures for the kaggle test set.\n","    \"\"\"\n","    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n","    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n","    sigs = calculate_signature(preprocessed, truncation_level=level).cpu()\n","    size = sigs.shape[1]\n","    sigs = sigs.reshape(-1,4,size)\n","    return sigs\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# input_utils.py"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:35:32.941932Z","iopub.status.busy":"2024-03-19T17:35:32.941502Z","iopub.status.idle":"2024-03-19T17:35:32.950887Z","shell.execute_reply":"2024-03-19T17:35:32.949707Z","shell.execute_reply.started":"2024-03-19T17:35:32.941892Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class KaggleTestDataset(Dataset):\n","    \"\"\"Labels are not available\"\"\"\n","    def __init__(self, metadata, features, mean, std):\n","        self.metadata = metadata\n","        self.mean = mean\n","        self.std = std\n","        self.features = self.preprocess_features(features, mean, std)\n","\n","\n","    def preprocess_features(self, features, mean, std):\n","        # fill na\n","        features = torch.nan_to_num(features)\n","        # normalize features\n","        features = (features - mean) / (std + 1e-6)\n","        features = torch.clamp(features, -3, 3)\n","\n","        return features.to(torch.float32)\n","    \n","    def __len__(self):\n","        return len(self.metadata)\n","    \n","    def __getitem__(self, idx):\n","        sample = self.features[idx]\n","        return sample"]},{"cell_type":"markdown","metadata":{},"source":["# model.py"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:24:41.647160Z","iopub.status.busy":"2024-03-19T17:24:41.646601Z","iopub.status.idle":"2024-03-19T17:24:41.659143Z","shell.execute_reply":"2024-03-19T17:24:41.657849Z","shell.execute_reply.started":"2024-03-19T17:24:41.647129Z"},"trusted":true},"outputs":[],"source":["class EnsembleModel(torch.nn.Module):\n","    def __init__(self, sig_dimension, dropout, classifier_input_dim, hidden_layer_dim):\n","        super(EnsembleModel, self).__init__()\n","        self.model = torch.nn.Sequential(\n","            torch.nn.Linear(sig_dimension, hidden_layer_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(hidden_layer_dim, classifier_input_dim),\n","            torch.nn.ReLU(),\n","        )\n","        self.classifier = torch.nn.Sequential(\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(classifier_input_dim*4, classifier_input_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(classifier_input_dim, classifier_input_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(classifier_input_dim, 6),\n","            torch.nn.Softmax(dim=1)\n","        )\n","    \n","    def forward(self, x):\n","        # put each brain region through the model, then concatenate and put into classifier\n","        outputs = []\n","        for i in range(4):\n","            outputs.append(self.model(x[:, i, :]))\n","        return self.classifier(torch.cat(outputs, axis=1))"]},{"cell_type":"markdown","metadata":{},"source":["# Main"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:41.700991Z","iopub.status.busy":"2024-03-19T17:46:41.699414Z","iopub.status.idle":"2024-03-19T17:46:41.705770Z","shell.execute_reply":"2024-03-19T17:46:41.704564Z","shell.execute_reply.started":"2024-03-19T17:46:41.700953Z"},"trusted":true},"outputs":[],"source":["import lightning as pl"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:42.455210Z","iopub.status.busy":"2024-03-19T17:46:42.454800Z","iopub.status.idle":"2024-03-19T17:46:42.460422Z","shell.execute_reply":"2024-03-19T17:46:42.459172Z","shell.execute_reply.started":"2024-03-19T17:46:42.455180Z"},"trusted":true},"outputs":[],"source":["TEST_METADATA_DIR    = \"../../data/test.csv\"\n","TEST_EEG_DIR         = \"../../data/test_eegs/\"\n","MODEL_DIR            = \"model_logs/0.0006365430731326342_0.00016552539309486747_0.5_512_256\""]},{"cell_type":"markdown","metadata":{},"source":["# Model Run"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:45.700384Z","iopub.status.busy":"2024-03-19T17:46:45.699703Z","iopub.status.idle":"2024-03-19T17:46:45.705544Z","shell.execute_reply":"2024-03-19T17:46:45.704417Z","shell.execute_reply.started":"2024-03-19T17:46:45.700336Z"},"trusted":true},"outputs":[],"source":["# hyperparameters \n","signature_level = 4\n","lr = 0.0007425105458353962\n","weight_decay = 3.305710445615262e-05\n","dropout = 0.5\n","early_stopping_epochs = 60\n","classifier_input_dim = 256\n","scaler_type = \"meanvar_1.0\"\n","logsigs_or_sigs = \"sigs\"\n","hidden_layer_dim = 128"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:48.277818Z","iopub.status.busy":"2024-03-19T17:46:48.277411Z","iopub.status.idle":"2024-03-19T17:46:48.622386Z","shell.execute_reply":"2024-03-19T17:46:48.620912Z","shell.execute_reply.started":"2024-03-19T17:46:48.277786Z"},"trusted":true},"outputs":[{"ename":"ValueError","evalue":"CUDA out of memory. Tried to allocate 5.09 GiB. GPU 0 has a total capacity of 11.73 GiB of which 176.12 MiB is free. Process 313385 has 1.17 GiB memory in use. Process 313899 has 1.16 GiB memory in use. Process 316130 has 3.84 GiB memory in use. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.10/site-packages/signatory/impl.py:36\u001b[0m, in \u001b[0;36m_wrap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.09 GiB. GPU 0 has a total capacity of 11.73 GiB of which 176.12 MiB is free. Process 313385 has 1.17 GiB memory in use. Process 313899 has 1.16 GiB memory in use. Process 316130 has 3.84 GiB memory in use. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(TEST_METADATA_DIR)\n\u001b[0;32m----> 3\u001b[0m signature_features \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_signature_for_metadata_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_EEG_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m (mean, std) \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mean_std.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m KaggleTestDataset(test_metadata, signature_features, mean, std)\n","Cell \u001b[0;32mIn[2], line 174\u001b[0m, in \u001b[0;36mcalculate_signature_for_metadata_test\u001b[0;34m(metadata, input_data_dir, scaler_type, device, level)\u001b[0m\n\u001b[1;32m    172\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n\u001b[1;32m    173\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(preprocessed, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 174\u001b[0m sigs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    175\u001b[0m size \u001b[38;5;241m=\u001b[39m sigs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    176\u001b[0m sigs \u001b[38;5;241m=\u001b[39m sigs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,size)\n","Cell \u001b[0;32mIn[2], line 153\u001b[0m, in \u001b[0;36mcalculate_signature\u001b[0;34m(preprocessed, truncation_level)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_signature\u001b[39m(preprocessed, truncation_level):\n\u001b[0;32m--> 153\u001b[0m     signature \u001b[38;5;241m=\u001b[39m \u001b[43msignatory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signature\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/signatory/signature_module.py:252\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(path, depth, stream, basepoint, inverse, initial, scalar_term)\u001b[0m\n\u001b[1;32m    250\u001b[0m result \u001b[38;5;241m=\u001b[39m _signature_batch_trick(path, depth, stream, basepoint, inverse, initial, scalar_term)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Either because we disabled use of the batch trick, or because the batch trick doesn't apply\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_SignatureFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# We have to do the transpose outside of autograd.Function.apply to avoid PyTorch bug 24413\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# NOT .transpose_ - the underlying TensorImpl (in C++) is used elsewhere and we don't want to change it.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/signatory/signature_module.py:61\u001b[0m, in \u001b[0;36m_SignatureFunction.forward\u001b[0;34m(ctx, path, depth, stream, basepoint, inverse, initial, scalar_term)\u001b[0m\n\u001b[1;32m     57\u001b[0m basepoint, basepoint_value \u001b[38;5;241m=\u001b[39m interpret_basepoint(basepoint, path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), path\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     58\u001b[0m                                                  path\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m initial, initial_value \u001b[38;5;241m=\u001b[39m interpret_initial(initial)\n\u001b[0;32m---> 61\u001b[0m signature_, path_increments \u001b[38;5;241m=\u001b[39m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(signature_, path_increments)\n\u001b[1;32m     64\u001b[0m ctx\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m=\u001b[39m depth\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/signatory/impl.py:38\u001b[0m, in \u001b[0;36m_wrap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n","\u001b[0;31mValueError\u001b[0m: CUDA out of memory. Tried to allocate 5.09 GiB. GPU 0 has a total capacity of 11.73 GiB of which 176.12 MiB is free. Process 313385 has 1.17 GiB memory in use. Process 313899 has 1.16 GiB memory in use. Process 316130 has 3.84 GiB memory in use. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_metadata = pd.read_csv(TEST_METADATA_DIR)\n","signature_features = calculate_signature_for_metadata_test(test_metadata, TEST_EEG_DIR, scaler_type, device, level=signature_level)\n","(mean, std) = torch.load(f\"{MODEL_DIR}/mean_std.pt\")\n","dataset = KaggleTestDataset(test_metadata, signature_features, mean, std)\n","test_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n","sig_dimension = dataset[0].shape[1]\n","\n","for i in range(4):\n","    model = EnsembleModel(sig_dimension, dropout, classifier_input_dim, hidden_layer_dim)\n","    model.load_state_dict(torch.load(f\"{MODEL_DIR}/model_{i}.pt\"))\n","    model.to(device)\n","    model.eval()\n","    predictions = []\n","    for batch in test_loader:\n","        batch = batch.to(device)\n","        predictions.append(model(batch).detach().cpu())\n","    predictions = torch.cat(predictions, axis=0)\n","    if i == 0:\n","        all_predictions = predictions\n","    else:\n","        all_predictions += predictions\n","\n","all_predictions /= all_predictions.sum(dim=1).unsqueeze(1)\n","submission = pd.DataFrame({\"eeg_id\": test_metadata.eeg_id.values})\n","submission[TARGETS] = all_predictions\n","submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4631192,"sourceId":7888618,"sourceType":"datasetVersion"},{"sourceId":167831394,"sourceType":"kernelVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
