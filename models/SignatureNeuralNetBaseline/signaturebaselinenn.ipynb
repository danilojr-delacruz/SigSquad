{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:22:52.506751Z","iopub.status.busy":"2024-03-19T17:22:52.506356Z","iopub.status.idle":"2024-03-19T17:24:38.047435Z","shell.execute_reply":"2024-03-19T17:24:38.046087Z","shell.execute_reply.started":"2024-03-19T17:22:52.506720Z"},"trusted":true},"outputs":[],"source":["! python -m pip install --no-index --find-links=../input/requirements -r ../input/requirements/requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["# utils.py"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:24:38.050291Z","iopub.status.busy":"2024-03-19T17:24:38.049926Z","iopub.status.idle":"2024-03-19T17:24:41.645243Z","shell.execute_reply":"2024-03-19T17:24:41.644237Z","shell.execute_reply.started":"2024-03-19T17:24:38.050257Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/barbora/.local/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n","Install h5py to use hdf5 features: http://docs.h5py.org/\n","  warn(h5py_msg)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tslearn.preprocessing import TimeSeriesScalerMinMax, TimeSeriesScalerMeanVariance\n","from scipy.signal import butter, lfilter\n","import signatory\n","import torch\n","\n","RESIDUAL_PAIRS = {'LP': [('Fp1', 'F3'), ('F3', 'C3'), ('C3', 'P3'), ('P3', 'O1')], \n","                  'RP': [('Fp2', 'F4'), ('F4', 'C4'), ('C4', 'P4'), ('P4', 'O2')], \n","                  'LT': [('Fp1', 'F7'), ('F7', 'T3'), ('T3', 'T5'), ('T5', 'O1')],\n","                  'RT': [('Fp2', 'F8'), ('F8', 'T4'), ('T4', 'T6'), ('T6', 'O2')],\n","                  # don't include the middle electrodes for now\n","                  #'C': [('Fz', 'Cz'), ('Cz', 'Pz')],\n","                  }\n","\n","TARGETS = [\n","    \"seizure_vote\",\t\"lpd_vote\", \"gpd_vote\",\n","    \"lrda_vote\", \"grda_vote\", \"other_vote\"\n","    ]\n","\n","def modify_metadata(metadata):\n","    \"\"\"Reduce the metadata to one data point per eeg_id (the one in the middle - median offset).\n","       Make the toal vote distribution across the reconding be the target.\n","       We are assuming that even though we have multiple sub-recordings, the true target value does not change.\n","    \"\"\"\n","    num_votes = metadata.iloc[:, -6:].sum(axis=1)\n","    metadata = metadata[num_votes >= 10]\n","    # note that other public notebooks calculate the offset differently, but I am not convinced it makes sense\n","    metadata_grouped = metadata.groupby(\"eeg_id\").agg(\n","        spectrogram_id     = pd.NamedAgg(\"spectrogram_id\", \"first\"),\n","        eeg_offset_seconds = pd.NamedAgg(\"eeg_label_offset_seconds\", \"median\"),\n","        spec_offset_seconds = pd.NamedAgg(\"spectrogram_label_offset_seconds\", \"median\"),\n","        patient_id         = pd.NamedAgg(\"patient_id\", \"first\"),\n","        target             = pd.NamedAgg(\"expert_consensus\", \"first\")\n","        )\n","    total_votes = metadata.groupby(\"eeg_id\")[TARGETS].agg(\"sum\")\n","    total_votes = total_votes.div(total_votes.sum(axis=1), axis=0)\n","    for vote_label in TARGETS:\n","        metadata_grouped[vote_label] = total_votes[vote_label]\n","\n","    return metadata_grouped.reset_index()\n","\n","def rescale(ts, scaler_type):\n","    \"\"\"Rescale the time series using the given type.\n","    \"\"\"\n","    if scaler_type == \"minmax\":\n","        scaler = TimeSeriesScalerMinMax()\n","        ts = scaler.fit_transform(ts)\n","    elif scaler_type.startswith(\"meanvar\"):\n","        scaler_std = float(scaler_type.split(\"_\")[1])\n","        scaler = TimeSeriesScalerMeanVariance(std=scaler_std)\n","        ts = scaler.fit_transform(ts)\n","    elif scaler_type.startswith(\"constant\"):\n","        scaler_constant = float(scaler_type.split(\"_\")[1])\n","        ts = ts / scaler_constant\n","    else:\n","        raise ValueError(f\"Unknown scaler type {scaler_type}\")\n","    return ts\n","\n","def transform_residuals(residuals, scaler_type):\n","    # clip very large outliers for the minmax scaler\n","    residuals = np.clip(residuals, -150, 150)\n","    residuals = rescale(residuals.values.reshape(1,-1,1), scaler_type).reshape(-1)\n","    return residuals\n","\n","\n","def get_residuals(eeg, scaler_type):\n","    \"\"\"Doctors look at the difference between two neighboring channels.\n","       Calculate the residuals for each channel pair.\n","       Group by brain region.\"\"\"\n","    brain_regions = []\n","    for region, pair in RESIDUAL_PAIRS.items():\n","        # include time as the first dimension and make it go from 0 to 1\n","        residuals = [np.linspace(0, 1, len(eeg))]\n","        for channel1, channel2 in pair:\n","            residual = transform_residuals(eeg[channel1] - eeg[channel2], scaler_type)\n","            residuals.append(residual)\n","        brain_regions.append(np.stack(residuals).T)\n","    return np.stack(brain_regions)\n","\n","\n","def butter_bandpass(lowcut, highcut, fs, order):\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","\n","def butter_bandpass_filter(data, lowcut=0.1, highcut=30, fs=200, order=4):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data, axis=0)\n","    return y\n","\n","def preprocess_for_sig(metadata, data_dir, scaler_type):\n","    \"\"\"\"Preprocess the eeg data to feed into the logsignature function.\n","        The output tensor is of the shape (paths_to_calculate x  path_length = 10000 x path_dimensions = 5).\n","        paths to calculate = number_of_eeg_recordings * 4 brain regions for each recording.\n","    \"\"\"\n","    preprocessed = []\n","    for i, data in metadata.iterrows():\n","        eeg_id = data.eeg_id\n","        # eeg is sampled at 200 Hz\n","        offset = int(data.eeg_offset_seconds * 200 )\n","        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n","        # clip outliers (eeg amplitudes are in the range of -100 to 100)\n","        eeg = pd.read_parquet(parquet_path).fillna(0).clip(-100,100)\n","        eeg = eeg.iloc[offset:offset+10000]\n","        # bandpass filter\n","        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n","        residuals = get_residuals(eeg, scaler_type)      \n","        preprocessed.append(residuals)\n","    preprocessed = np.concatenate(preprocessed, axis=0)\n","    \n","    return preprocessed\n","\n","def preprocess_for_sig_test(metadata, data_dir, scaler_type):\n","    \"\"\"Preprocessing needs to be different for the kaggle test set since we only have 50 second eeg recordings.\"\"\"\n","    preprocessed = []\n","    for i, data in metadata.iterrows():\n","        eeg_id = data.eeg_id\n","        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n","        eeg = pd.read_parquet(parquet_path).fillna(0).clip(-100,100)\n","        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n","        residuals = get_residuals(eeg, scaler_type)      \n","        preprocessed.append(residuals)\n","    preprocessed = np.concatenate(preprocessed, axis=0)\n","\n","    return preprocessed\n","\n","def calculate_logsignature(preprocessed, truncation_level):\n","    logsignature = signatory.logsignature(preprocessed, truncation_level)\n","    return logsignature\n","\n","def calculate_signature(preprocessed, truncation_level):\n","    signature = signatory.signature(preprocessed, truncation_level)\n","    return signature\n","\n","def calculate_logsignature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n","    \"\"\"Return the tensor of calculated signtures.\n","       Use this function to calculate the logsignatures for the kaggle test set.\n","    \"\"\"\n","    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n","    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n","    logsigs = calculate_logsignature(preprocessed, truncation_level=level).cpu()\n","    size = logsigs.shape[1]\n","    logsigs = logsigs.reshape(-1,4,size)\n","    return logsigs\n","\n","\n","def calculate_signature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n","    \"\"\"Return the tensor of calculated signtures.\n","       Use this function to calculate the signatures for the kaggle test set.\n","    \"\"\"\n","    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n","    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n","    sigs = calculate_signature(preprocessed, truncation_level=level).cpu()\n","    size = sigs.shape[1]\n","    sigs = sigs.reshape(-1,4,size)\n","    return sigs\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# input_utils.py"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:35:32.941932Z","iopub.status.busy":"2024-03-19T17:35:32.941502Z","iopub.status.idle":"2024-03-19T17:35:32.950887Z","shell.execute_reply":"2024-03-19T17:35:32.949707Z","shell.execute_reply.started":"2024-03-19T17:35:32.941892Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class KaggleTestDataset(Dataset):\n","    \"\"\"Labels are not available\"\"\"\n","    def __init__(self, metadata, features, mean, std):\n","        self.metadata = metadata\n","        self.mean = mean\n","        self.std = std\n","        self.features = self.preprocess_features(features, mean, std)\n","\n","\n","    def preprocess_features(self, features, mean, std):\n","        # fill na\n","        features = torch.nan_to_num(features)\n","        # normalize features\n","        features = (features - mean) / (std + 1e-6)\n","        features = torch.clamp(features, -3, 3)\n","\n","        return features.to(torch.float32)\n","    \n","    def __len__(self):\n","        return len(self.metadata)\n","    \n","    def __getitem__(self, idx):\n","        sample = self.features[idx]\n","        return sample"]},{"cell_type":"markdown","metadata":{},"source":["# model.py"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:24:41.647160Z","iopub.status.busy":"2024-03-19T17:24:41.646601Z","iopub.status.idle":"2024-03-19T17:24:41.659143Z","shell.execute_reply":"2024-03-19T17:24:41.657849Z","shell.execute_reply.started":"2024-03-19T17:24:41.647129Z"},"trusted":true},"outputs":[],"source":["class EnsembleModel(torch.nn.Module):\n","    def __init__(self, sig_dimension, dropout, classifier_input_dim, hidden_layer_dim):\n","        super(EnsembleModel, self).__init__()\n","        self.model = torch.nn.Sequential(\n","            torch.nn.Linear(sig_dimension, hidden_layer_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(hidden_layer_dim, classifier_input_dim),\n","            torch.nn.ReLU(),\n","        )\n","        self.classifier = torch.nn.Sequential(\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(classifier_input_dim*4, classifier_input_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(classifier_input_dim, classifier_input_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(classifier_input_dim, 6),\n","            torch.nn.Softmax(dim=1)\n","        )\n","    \n","    def forward(self, x):\n","        # put each brain region through the model, then concatenate and put into classifier\n","        outputs = []\n","        for i in range(4):\n","            outputs.append(self.model(x[:, i, :]))\n","        return self.classifier(torch.cat(outputs, axis=1))"]},{"cell_type":"markdown","metadata":{},"source":["# Main"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:41.700991Z","iopub.status.busy":"2024-03-19T17:46:41.699414Z","iopub.status.idle":"2024-03-19T17:46:41.705770Z","shell.execute_reply":"2024-03-19T17:46:41.704564Z","shell.execute_reply.started":"2024-03-19T17:46:41.700953Z"},"trusted":true},"outputs":[],"source":["import lightning as pl"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:42.455210Z","iopub.status.busy":"2024-03-19T17:46:42.454800Z","iopub.status.idle":"2024-03-19T17:46:42.460422Z","shell.execute_reply":"2024-03-19T17:46:42.459172Z","shell.execute_reply.started":"2024-03-19T17:46:42.455180Z"},"trusted":true},"outputs":[],"source":["TEST_METADATA_DIR    = \"../../data/test.csv\"\n","TEST_EEG_DIR         = \"../../data/test_eegs/\"\n","MODEL_DIR            = \"model_logs/0.0006365430731326342_0.00016552539309486747_0.5_512_256\""]},{"cell_type":"markdown","metadata":{},"source":["# Model Run"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:45.700384Z","iopub.status.busy":"2024-03-19T17:46:45.699703Z","iopub.status.idle":"2024-03-19T17:46:45.705544Z","shell.execute_reply":"2024-03-19T17:46:45.704417Z","shell.execute_reply.started":"2024-03-19T17:46:45.700336Z"},"trusted":true},"outputs":[],"source":["# hyperparameters \n","signature_level = 4\n","lr = 0.0007425105458353962\n","weight_decay = 3.305710445615262e-05\n","dropout = 0.5\n","early_stopping_epochs = 60\n","classifier_input_dim = 256\n","scaler_type = \"meanvar_1.0\"\n","logsigs_or_sigs = \"sigs\"\n","hidden_layer_dim = 128"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:46:48.277818Z","iopub.status.busy":"2024-03-19T17:46:48.277411Z","iopub.status.idle":"2024-03-19T17:46:48.622386Z","shell.execute_reply":"2024-03-19T17:46:48.620912Z","shell.execute_reply.started":"2024-03-19T17:46:48.277786Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["preprocessed\n","Model 0 loaded\n","Model 0 predictions done\n","Model 1 loaded\n","Model 1 predictions done\n","Model 2 loaded\n","Model 2 predictions done\n","Model 3 loaded\n","Model 3 predictions done\n","Predictions done\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_metadata = pd.read_csv(TEST_METADATA_DIR)\n","signature_features = calculate_signature_for_metadata_test(test_metadata, TEST_EEG_DIR, scaler_type, device, level=signature_level)\n","(mean, std) = torch.load(f\"{MODEL_DIR}/mean_std.pt\")\n","dataset = KaggleTestDataset(test_metadata, signature_features, mean, std)\n","test_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n","sig_dimension = dataset[0].shape[1]\n","\n","for i in range(4):\n","    model = EnsembleModel(sig_dimension, dropout, classifier_input_dim, hidden_layer_dim)\n","    model.load_state_dict(torch.load(f\"{MODEL_DIR}/model_{i}.pt\"))\n","    model.to(device)\n","    model.eval()\n","    predictions = []\n","    for batch in test_loader:\n","        batch = batch.to(device)\n","        predictions.append(model(batch).detach().cpu())\n","    predictions = torch.cat(predictions, axis=0)\n","    if i == 0:\n","        all_predictions = predictions\n","    else:\n","        all_predictions += predictions\n","\n","all_predictions /= all_predictions.sum(dim=1).unsqueeze(1)\n","submission = pd.DataFrame({\"eeg_id\": test_metadata.eeg_id.values})\n","submission[TARGETS] = all_predictions\n","submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4631192,"sourceId":7888618,"sourceType":"datasetVersion"},{"sourceId":167831394,"sourceType":"kernelVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
