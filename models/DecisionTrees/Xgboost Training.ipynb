{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import modify_metadata, TARGETS\n",
    "import torch\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from ax.service.managed_loop import optimize\n",
    "import cupy as cp\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_METADATA_DIR = \"../../data/train.csv\"\n",
    "TRAIN_SIGNATURES_DIR = \"../../data/train_signatures/\"\n",
    "metadata = pd.read_csv(TRAIN_METADATA_DIR)\n",
    "metadata = modify_metadata(metadata)\n",
    "criterion = torch.nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_level = 4\n",
    "scaler_type = \"meanvarPerChannel_1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIGNATURES_FILE = f\"{TRAIN_SIGNATURES_DIR}all_sigs_lvl_{signature_level}_scaler_{scaler_type}_experts.pt\"\n",
    "signature_features = torch.load(TRAIN_SIGNATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = metadata[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = signature_features.reshape(signature_features.shape[0], -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"name\": \"n_estimators\", \"type\": \"range\", \"bounds\": [1, 1000]},\n",
    "    {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": [3, 8]},\n",
    "    {\"name\": \"min_child_weight\", \"type\": \"range\", \"bounds\": [1, 10]},\n",
    "    {\"name\": \"gamma\", \"type\": \"range\", \"bounds\": [0., 0.5]},\n",
    "    {\"name\": \"subsample\", \"type\": \"range\", \"bounds\": [0.5, 1.]},\n",
    "    {\"name\": \"colsample_bytree\", \"type\": \"range\", \"bounds\": [0.5, 1.]},\n",
    "    {\"name\": \"eta\", \"type\": \"range\", \"bounds\": [0.01, 0.3]},\n",
    "    {\"name\": \"lambda\", \"type\": \"range\", \"bounds\": [0., 100.]},\n",
    "    {\"name\": \"alpha\", \"type\": \"range\", \"bounds\": [0., 100.]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_score(parameters, save_models = False):\n",
    "    print(parameters)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    #multiprocessing.set_start_method('spawn', force=True)\n",
    "    for i, (train_index, valid_index) in enumerate(gkf.split(metadata, metadata.target, metadata.patient_id)):\n",
    "        X_train, X_valid = np.take(features, train_index, axis=0), np.take(features, valid_index, axis=0)\n",
    "        y_train, y_valid = np.take(targets, train_index, axis=0), np.take(targets, valid_index, axis=0)\n",
    "        y_train_argmax = np.argmax(y_train, axis=1)\n",
    "        # put on GPU\n",
    "        X_train = cp.array(X_train)\n",
    "        y_train_argmax = cp.array(y_train_argmax)\n",
    "        clf = xgb.XGBClassifier(tree_method = \"hist\", device = \"cuda\", n_estimators = parameters.get(\"n_estimators\"), max_depth = parameters.get(\"max_depth\"), min_child_weight = parameters.get(\"min_child_weight\"), gamma = parameters.get(\"gamma\"), subsample = parameters.get(\"subsample\"), colsample_bytree = parameters.get(\"colsample_bytree\"), eta = parameters.get(\"eta\"), reg_lambda = parameters.get(\"lambda\"), reg_alpha = parameters.get(\"alpha\"))\n",
    "        clf.fit(X_train, y_train_argmax)\n",
    "        X_valid = cp.array(X_valid)\n",
    "        y_pred = clf.predict_proba(X_valid)\n",
    "        loss = criterion(torch.log(torch.tensor(y_pred)), torch.tensor(y_valid))\n",
    "        print(f\"Fold {i} loss: {loss.item()}\")\n",
    "        scores.append(loss.item())\n",
    "        if save_models:\n",
    "            # create model directory\n",
    "            dir_name = f\"model_logs/xgboost_{parameters.get('n_estimators')}_{parameters.get('max_depth')}_{parameters.get('min_child_weight')}_{parameters.get('gamma')}_{parameters.get('subsample')}_{parameters.get('colsample_bytree')}_{parameters.get('eta')}_{parameters.get('lambda')}_{parameters.get('alpha')}\"\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "            clf.save_model(f\"{dir_name}/model_{i}.json\")\n",
    "    # write scores to file\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log file for all experiments and CV scores\n",
    "log_file = \"hyperparameter_search_log.csv\"\n",
    "if not os.path.exists(log_file):\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(\"n_estimators,max_depth,min_child_weight,gamma,subsample,colsample_bytree,eta,lambda,alpha,loss\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(parameters):\n",
    "    scores = CV_score(parameters)\n",
    "    # read scores from file\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{parameters.get('n_estimators')},{parameters.get('max_depth')},{parameters.get('min_child_weight')},{parameters.get('gamma')},{parameters.get('subsample')},{parameters.get('colsample_bytree')},{parameters.get('eta')},{parameters.get('lambda')},{parameters.get('alpha')},{np.mean(scores)}\\n\")\n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_function({'n_estimators': 510, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.06360600612445994, 'subsample': 0.7326052348306491, 'colsample_bytree': 0.7798921755137922, 'eta': 0.01, 'lambda': 0.5856295363545664, 'alpha': 0.4046439680543726})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=eval_function,\n",
    "    objective_name='CV_score',\n",
    "    total_trials=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on best parameters\n",
    "CV_score({'n_estimators': 510, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.06360600612445994, 'subsample': 0.7326052348306491, 'colsample_bytree': 0.7798921755137922, 'eta': 0.01, 'lambda': 0.5856295363545664, 'alpha': 0.4046439680543726}, save_models = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
