{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install --no-index --find-links=../input/requirements -r ../input/requirements/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barbora/.local/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax, TimeSeriesScalerMeanVariance\n",
    "from scipy.signal import butter, lfilter\n",
    "import signatory\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "RESIDUAL_PAIRS = {'LP': [('Fp1', 'F3'), ('F3', 'C3'), ('C3', 'P3'), ('P3', 'O1')], \n",
    "                  'RP': [('Fp2', 'F4'), ('F4', 'C4'), ('C4', 'P4'), ('P4', 'O2')], \n",
    "                  'LT': [('Fp1', 'F7'), ('F7', 'T3'), ('T3', 'T5'), ('T5', 'O1')],\n",
    "                  'RT': [('Fp2', 'F8'), ('F8', 'T4'), ('T4', 'T6'), ('T6', 'O2')],\n",
    "                  # don't include the middle electrodes for now\n",
    "                  #'C': [('Fz', 'Cz'), ('Cz', 'Pz')],\n",
    "                  }\n",
    "\n",
    "TARGETS = [\n",
    "    \"seizure_vote\",\t\"lpd_vote\", \"gpd_vote\",\n",
    "    \"lrda_vote\", \"grda_vote\", \"other_vote\"\n",
    "    ]\n",
    "\n",
    "def modify_metadata(metadata):\n",
    "    \"\"\"Reduce the metadata to one data point per eeg_id (the one in the middle - median offset).\n",
    "       Make the toal vote distribution across the reconding be the target.\n",
    "       We are assuming that even though we have multiple sub-recordings, the true target value does not change.\n",
    "    \"\"\"\n",
    "    num_votes = metadata.iloc[:, -6:].sum(axis=1)\n",
    "    metadata = metadata[(num_votes >= 10)]\n",
    "    # note that other public notebooks calculate the offset differently, but I am not convinced it makes sense\n",
    "    metadata_grouped = metadata.groupby(\"eeg_id\").agg(\n",
    "        spectrogram_id     = pd.NamedAgg(\"spectrogram_id\", \"first\"),\n",
    "        eeg_offset_seconds = pd.NamedAgg(\"eeg_label_offset_seconds\", \"median\"),\n",
    "        spec_offset_seconds = pd.NamedAgg(\"spectrogram_label_offset_seconds\", \"median\"),\n",
    "        patient_id         = pd.NamedAgg(\"patient_id\", \"first\"),\n",
    "        target             = pd.NamedAgg(\"expert_consensus\", \"first\")\n",
    "        )\n",
    "    total_votes = metadata.groupby(\"eeg_id\")[TARGETS].agg(\"sum\")\n",
    "    total_votes = total_votes.div(total_votes.sum(axis=1), axis=0)\n",
    "    for vote_label in TARGETS:\n",
    "        metadata_grouped[vote_label] = total_votes[vote_label]\n",
    "\n",
    "    return metadata_grouped.reset_index()\n",
    "\n",
    "def rescale(ts, scaler_type):\n",
    "    \"\"\"Rescale the time series using the given type.\n",
    "    \"\"\"\n",
    "    if scaler_type == \"minmax\":\n",
    "        scaler = TimeSeriesScalerMinMax()\n",
    "        ts = scaler.fit_transform(ts)\n",
    "    elif scaler_type.startswith(\"meanvarPerChannel\"):\n",
    "        scaler_std = float(scaler_type.split(\"_\")[1])\n",
    "        scaler = TimeSeriesScalerMeanVariance(std=scaler_std)\n",
    "        ts = scaler.fit_transform(ts, std=scaler_std)\n",
    "    elif scaler_type.startswith(\"constant\"):\n",
    "        scaler_constant = float(scaler_type.split(\"_\")[1])\n",
    "        ts = ts / scaler_constant\n",
    "    elif scaler_type.startswith(\"meanvar\"):\n",
    "        # this is done later since we atke the variance across all channels\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaler type {scaler_type}\")\n",
    "    return ts\n",
    "\n",
    "def transform_residuals(residuals, scaler_type):\n",
    "    residuals = rescale(residuals.values.reshape(1,-1,1), scaler_type).reshape(-1)\n",
    "    return residuals\n",
    "\n",
    "def get_residuals(eeg, scaler_type):\n",
    "    \"\"\"Doctors look at the difference between two neighboring channels.\n",
    "       Calculate the residuals for each channel pair.\n",
    "       Group by brain region.\"\"\"\n",
    "    brain_regions = []\n",
    "    for region, pair in RESIDUAL_PAIRS.items():\n",
    "        # include time as the first dimension and make it go from 0 to 1\n",
    "        residuals = []\n",
    "        for channel1, channel2 in pair:\n",
    "            residual = transform_residuals(eeg[channel1] - eeg[channel2], scaler_type)\n",
    "            residuals.append(residual)\n",
    "        brain_regions.append(np.stack(residuals).T)\n",
    "    brain_regions = np.stack(brain_regions)\n",
    "\n",
    "    if scaler_type.startswith(\"meanvar\"):\n",
    "        brain_regions = brain_regions - brain_regions.mean(axis=1, keepdims=True)\n",
    "        brain_regions = brain_regions / (brain_regions.std()+1e-6)\n",
    "    return brain_regions.clip(-4, 4)\n",
    "\n",
    "def augment_with_time(residuals):\n",
    "    \"\"\" take residuals of the shape (4, 10000, 4) and augment with time to obtain (4, 10000, 5)\"\"\"\n",
    "    augmented_regions = []\n",
    "    for region_index in range(4):\n",
    "        augmented_regions.append(np.concatenate([residuals[region_index], np.linspace(0,1,10000).reshape(-1,1)], axis=1))\n",
    "    return np.stack(augmented_regions)\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut=0.1, highcut=30, fs=200, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data, axis=0)\n",
    "    return y\n",
    "\n",
    "def preprocess_for_sig(metadata, data_dir, scaler_type):\n",
    "    \"\"\"\"Preprocess the eeg data to feed into the logsignature function.\n",
    "        The output tensor is of the shape (paths_to_calculate x  path_length = 10000 x path_dimensions = 5).\n",
    "        paths to calculate = number_of_eeg_recordings * 4 brain regions for each recording.\n",
    "    \"\"\"\n",
    "    preprocessed = []\n",
    "    for i, data in metadata.iterrows():\n",
    "        eeg_id = data.eeg_id\n",
    "        # eeg is sampled at 200 Hz\n",
    "        offset = int(data.eeg_offset_seconds * 200 )\n",
    "        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n",
    "        eeg = pd.read_parquet(parquet_path)\n",
    "        # replace 9999 with 0\n",
    "        eeg = eeg.replace(9999, 0)\n",
    "        eeg = eeg.fillna(0).clip(-1000,1000)\n",
    "        eeg = eeg.iloc[offset:offset+10000]\n",
    "        # bandpass filter\n",
    "        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n",
    "        residuals = get_residuals(eeg, scaler_type)\n",
    "        residuals = augment_with_time(residuals)      \n",
    "        preprocessed.append(residuals)\n",
    "    preprocessed = np.concatenate(preprocessed, axis=0)\n",
    "    \n",
    "    return preprocessed\n",
    "\n",
    "def preprocess_for_sig_test(metadata, data_dir, scaler_type):\n",
    "    \"\"\"Preprocessing needs to be different for the kaggle test set since we only have 50 second eeg recordings.\"\"\"\n",
    "    preprocessed = []\n",
    "    for i, data in metadata.iterrows():\n",
    "        eeg_id = data.eeg_id\n",
    "        parquet_path = (f\"{data_dir}{eeg_id}.parquet\")\n",
    "        eeg = pd.read_parquet(parquet_path)\n",
    "        # replace 9999 with 0\n",
    "        eeg = eeg.replace(9999, 0)\n",
    "        eeg = eeg.fillna(0).clip(-1000,1000)\n",
    "        eeg = pd.DataFrame(butter_bandpass_filter(eeg), columns=eeg.columns)\n",
    "        residuals = get_residuals(eeg, scaler_type)\n",
    "        residuals = augment_with_time(residuals)      \n",
    "        preprocessed.append(residuals)\n",
    "    preprocessed = np.concatenate(preprocessed, axis=0)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "def calculate_logsignature(preprocessed, truncation_level):\n",
    "    logsignature = signatory.logsignature(preprocessed, truncation_level)\n",
    "    return logsignature\n",
    "\n",
    "def calculate_signature(preprocessed, truncation_level):\n",
    "    signature = signatory.signature(preprocessed, truncation_level)\n",
    "    return signature\n",
    "\n",
    "def calculate_logsignature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n",
    "    \"\"\"Return the tensor of calculated signtures.\n",
    "       Use this function to calculate the logsignatures for the kaggle test set.\n",
    "    \"\"\"\n",
    "    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n",
    "    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n",
    "    logsigs = calculate_logsignature(preprocessed, truncation_level=level).cpu()\n",
    "    size = logsigs.shape[1]\n",
    "    logsigs = logsigs.reshape(-1,4,size)\n",
    "    return logsigs\n",
    "\n",
    "\n",
    "def calculate_signature_for_metadata_test(metadata, input_data_dir, scaler_type, device=\"cpu\", level=5):\n",
    "    \"\"\"Return the tensor of calculated signtures.\n",
    "       Use this function to calculate the signatures for the kaggle test set.\n",
    "    \"\"\"\n",
    "    preprocessed = preprocess_for_sig_test(metadata, input_data_dir, scaler_type)\n",
    "    preprocessed = torch.tensor(preprocessed, dtype=torch.float64).to(device)\n",
    "    sigs = calculate_signature(preprocessed, truncation_level=level).cpu()\n",
    "    size = sigs.shape[1]\n",
    "    sigs = sigs.reshape(-1,4,size)\n",
    "    return sigs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_METADATA_DIR    = \"../../data/test.csv\"\n",
    "TEST_EEG_DIR         = \"../../data/test_eegs/\"\n",
    "MODEL_DIR            = \"model_logs/0.0006365430731326342_0.00016552539309486747_0.5_512_256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_estimators = 510 \n",
    "max_depth = 4\n",
    "min_child_weight = 9\n",
    "gamma = 0.06360600612445994\n",
    "subsample = 0.7326052348306491\n",
    "colsample_bytree = 0.7798921755137922\n",
    "eta = 0.01\n",
    "reg_lambda = 0.5856295363545664\n",
    "reg_alpha = 0.4046439680543726\n",
    "signature_level = 4\n",
    "scaler_type = \"meanvarPerChannel_1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_metadata = pd.read_csv(TEST_METADATA_DIR)\n",
    "signature_features = calculate_signature_for_metadata_test(test_metadata, TEST_EEG_DIR, scaler_type, device, level=signature_level)\n",
    "\n",
    "for i in range(5):\n",
    "    clf = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, min_child_weight=min_child_weight, gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree, eta=eta, reg_lambda=reg_lambda, reg_alpha=reg_alpha, objective=\"multi:softprob\", num_class=6, tree_method=\"gpu_hist\", gpu_id=0)\n",
    "    clf.load_model(f\"{MODEL_DIR}/model_{i}.json\")\n",
    "    predictions = clf.predict_proba(signature_features)\n",
    "    if i == 0:\n",
    "        all_predictions = predictions\n",
    "    else:\n",
    "        all_predictions += predictions\n",
    "\n",
    "all_predictions /= all_predictions.sum(dim=1).unsqueeze(1)\n",
    "submission = pd.DataFrame({\"eeg_id\": test_metadata.eeg_id.values})\n",
    "submission[TARGETS] = all_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
